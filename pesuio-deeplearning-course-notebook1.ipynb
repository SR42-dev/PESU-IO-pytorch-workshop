{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"'''\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n'''","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-23T09:42:57.620274Z","iopub.execute_input":"2021-10-23T09:42:57.620637Z","iopub.status.idle":"2021-10-23T09:42:57.62839Z","shell.execute_reply.started":"2021-10-23T09:42:57.620603Z","shell.execute_reply":"2021-10-23T09:42:57.627721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Linear regression using pytorch\n\nimport torch as tc\nimport numpy as np\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2021-10-23T09:42:57.629661Z","iopub.execute_input":"2021-10-23T09:42:57.62988Z","iopub.status.idle":"2021-10-23T09:42:57.64286Z","shell.execute_reply.started":"2021-10-23T09:42:57.629853Z","shell.execute_reply":"2021-10-23T09:42:57.641742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N = 100\nX = np.random.random(N)*10 - 5\n\nw = 0.5 # aim is to find w & b such that\nb = 1\n\nY = w*X - b + np.random.random(N) # X & noise components are randomized","metadata":{"execution":{"iopub.status.busy":"2021-10-23T09:42:57.644234Z","iopub.execute_input":"2021-10-23T09:42:57.644574Z","iopub.status.idle":"2021-10-23T09:42:57.655688Z","shell.execute_reply.started":"2021-10-23T09:42:57.644522Z","shell.execute_reply":"2021-10-23T09:42:57.654973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.scatter(X,Y) # predictable trends here result in lower losses during training phase","metadata":{"execution":{"iopub.status.busy":"2021-10-23T09:42:57.65882Z","iopub.execute_input":"2021-10-23T09:42:57.65998Z","iopub.status.idle":"2021-10-23T09:42:58.101018Z","shell.execute_reply.started":"2021-10-23T09:42:57.659911Z","shell.execute_reply":"2021-10-23T09:42:58.100151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = tc.nn.Linear(1,1) # linear neural network with 1 i/p & 1 o/p","metadata":{"execution":{"iopub.status.busy":"2021-10-23T09:42:58.102182Z","iopub.execute_input":"2021-10-23T09:42:58.102412Z","iopub.status.idle":"2021-10-23T09:42:58.107122Z","shell.execute_reply.started":"2021-10-23T09:42:58.102384Z","shell.execute_reply":"2021-10-23T09:42:58.106024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# mean squared error loss function\ne_func = tc.nn.MSELoss()\noptim = tc.optim.SGD(model.parameters(), lr = 0.001) # returns model parameters (learnables, independent variables) & lr-> learning rate (higher lr may cause increasing losses during training)","metadata":{"execution":{"iopub.status.busy":"2021-10-23T09:42:58.108382Z","iopub.execute_input":"2021-10-23T09:42:58.108735Z","iopub.status.idle":"2021-10-23T09:42:58.121122Z","shell.execute_reply.started":"2021-10-23T09:42:58.108707Z","shell.execute_reply":"2021-10-23T09:42:58.120156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# converting arrays into tensors (vectors compatible with pytorch)\nX = X.reshape(N, 1) # converts to array of N,1 dimensions\nY = Y.reshape(N,1) # default array can't be any shape but N,1\n\nx = tc.from_numpy(X.astype(np.float32))\ny = tc.from_numpy(Y.astype(np.float32))","metadata":{"execution":{"iopub.status.busy":"2021-10-23T09:42:58.122656Z","iopub.execute_input":"2021-10-23T09:42:58.123186Z","iopub.status.idle":"2021-10-23T09:42:58.134411Z","shell.execute_reply.started":"2021-10-23T09:42:58.123142Z","shell.execute_reply":"2021-10-23T09:42:58.133578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model training\n\nep = 1000 # epochs\n\nfor e in range(ep) :\n    optim.zero_grad() # explicitly setting network gradients to zero\n    \n    output = model(x) # x is the input tensor \n    loss = e_func(output, y)\n    print('Loss : ', loss.item())\n    \n    loss.backward() # updates the gradients with newly calculated values (dE/dw, dE/db)\n    optim.step() # w = w - (learningRate*(dE/dW))","metadata":{"execution":{"iopub.status.busy":"2021-10-23T09:42:58.137032Z","iopub.execute_input":"2021-10-23T09:42:58.137353Z","iopub.status.idle":"2021-10-23T09:42:58.81156Z","shell.execute_reply.started":"2021-10-23T09:42:58.137312Z","shell.execute_reply":"2021-10-23T09:42:58.810728Z"},"trusted":true},"execution_count":null,"outputs":[]}]}